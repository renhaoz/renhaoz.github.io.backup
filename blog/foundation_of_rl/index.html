<!DOCTYPE html>

<html lang="en-us"><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css"
        integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Nanum+Myeongjo&family=Noto+Serif+JP&family=Cormorant+Garamond&family=Libre+Baskerville&family=Source+Serif+Pro&family=Crimson+Text&family=Inter&family=Crimson+Pro&family=Literata&family=Ubuntu+Mono&family=Inter&family=Roboto">
    <link rel="stylesheet" type="text/css" href="/css/style.css">

    
    

    <title>renhaoz | Foundation of RL</title>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA\/G-123123-12', 'auto');
	
	ga('send', 'pageview');
}
</script>


</head><body class="container d-flex flex-column min-vh-100">

<div class="blog_nav_bar secondary_font ">
    
    
    <a class="navbar-brand" href="/">about</a>
    
    
    
    <a class="navbar-brand" href="/blog">« all posts</a>
    
    
</div>



<h3>
    <a class="title" href="/blog/foundation_of_rl/">Foundation of RL</a>
</h3>

<div class="reading_time secondary_font text-muted ">
    <span>
        Aug 25 2021 · 8 min read
    </span>

</div>




<div class="tags_navigation">
    
    <a class="tag" href="/tags/rl/">#rl</a>
    
</div>


<div class="toc ">
    <div>
        <strong>Table of Contents</strong>
    </div>
    <div>
        
        <nav id="TableOfContents">
  <ul>
    <li><a href="#foundation-of-rl">Foundation of RL</a>
      <ul>
        <li>
          <ul>
            <li><a href="#1-markov-decision-process-mdp">1. Markov Decision Process, MDP</a></li>
            <li><a href="#2-dynamic-process-dp">2. Dynamic Process, DP</a></li>
            <li><a href="#3-monte-carlo-methods-mc">3. Monte Carlo Methods, MC</a></li>
            <li><a href="#4-temporal-difference-td">4. Temporal Difference, TD</a></li>
          </ul>
        </li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
</div>
<h1 id="foundation-of-rl">Foundation of RL</h1>
<p>This blog will briefly introduce Markov decision process (MDP), dynamic process (DP), Monte Carlo methods (MC), temporal difference (TD).</p>
<h3 id="1-markov-decision-process-mdp">1. Markov Decision Process, MDP</h3>
<p>A MDP is defined by:</p>
<ul>
<li>Set of states, $S$</li>
<li>Set of actions, $A$</li>
<li>Transition function, $P(s'|s,a)$</li>
<li>Reward function, $R(s,a,s&rsquo;)$</li>
<li>Start state, $s_0$</li>
<li>Discount factor, $\gamma$</li>
<li>Horizon, $H$, especially when the MDP is infinite, $H \rightarrow \infty$</li>
</ul>
<p><img src="./images/MDP.png" alt=""></p>
<p>At each epoch $t$, an agent would make action $a_t$ based on the state it observed $s_t$ with its policy $\pi$, where $a_t=\pi(s_t)$.</p>
<p>Then the environment would change its state to $s_{t+1}$ and the agent would get its current reward $r_t$, according to the transition function $P(s'|s,a)$ and reward function $R(s,a,s&rsquo;)$ respectively.</p>
<p>The goal of the agent is to learn a policy $\pi$ to maximize its expected discounted reward $\Bbb{E}<em>\pi[G_t]$, where $G_t=\sum</em>{k=0}^{H}\gamma^kr_{t+1+k}=r_{t+1}+\gamma G_{t+1}$. *</p>
<p>The value of state $s$ under policy $\pi$ is defined as the expectation of the expected discounted reward when starting in $s$ and following $\pi$ thereafter, where $v_\pi(s)=\Bbb{E}_\pi(G_t|s)$ is the **state-value**.</p>
<p>The value of taking action $a$ in state $s$ under policy $\pi$ is defined as the expectation of the expected discounted reward when starting in $s$  taking action $a$ and following $\pi$ thereafter, where $q_\pi(s,a)=\Bbb{E}_\pi(G_t|s,a)$ is the **action-value**.</p>
<p>We can solve the function recursively, then we&rsquo;ll get the <strong>Bellman function</strong>:
$$
v_\pi(s)=\sum_a \pi(a|s) \sum_{s&rsquo;,r} p(s&rsquo;,r|s,a)[r+\gamma v_\pi(s&rsquo;)]
$$
Then we can get the relationship between $v_\pi(s)$ and $q_\pi(s,a)$:
$$
q_\pi(s,a)=\sum_{s&rsquo;,r} p(s&rsquo;,r|s,a)[r+\gamma v_\pi(s&rsquo;)] \<br>
v_\pi(s)=\sum_a \pi(a|s) q_\pi(s,a)
$$
To achieve our goal which is to maximize the $\Bbb{E}_\pi(G)$, we have to find the optimal state-value function $v_*(s)$ or optimal action-value function $q_*(s,a)$, where:
$$
v_*(s)=\underset{\pi}{max} , v_\pi(s)=\underset{a}{max} , \sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)[r+\gamma , v_*(s&rsquo;)] \<br>
q_*(s)=\underset{\pi}{max} , q_\pi(s,a)=\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)[r+\gamma , \underset{a&rsquo;}{max} , q_*(s&rsquo;,a&rsquo;)]
$$</p>
<p>So we have to find optimal policy for the agent with different algorithms.</p>
<p>* <strong>Maximum Entropy Formulation</strong>: Practically, sometime we don&rsquo;t want the policy to output one best action because the environment may change. Instead, we want to find the distribution over near-optimal solutions, which would introduce a concept of <strong>Maximum Entropy Formulation</strong>. Instead of maximizing $\Bbb{E}[G_t]$, we maximize $\Bbb{E}[G_t+\beta \cal{H}(\pi(\cdot|s_t))]$, where $\cal{H}(\cdot)$ is the entropy. If the policy is deterministic, the entropy is 0. So this approach will help the policy to output a distribution.</p>
<h3 id="2-dynamic-process-dp">2. Dynamic Process, DP</h3>
<p>DP algorithm can be used to find the optimal policy and it can be achieved by <strong>value iteration</strong> or <strong>policy iteration</strong>.</p>
<h5 id="21-policy-iteration">2.1 Policy Iteration</h5>
<p>Policy iteration includes two phases, namely policy evaluation and policy improvement.</p>
<p><strong>Policy evaluation</strong> has to find out the current state-value which is hard to directly solve the Bellman equation, so we use calculate its numerical solution:</p>
<blockquote>
<p>Start with $V_0(s)=0$ for all $s$</p>
<p>For each $s \in S$</p>
<p>​	$k=1$</p>
<p>​	Repeat:</p>
<p>​		$V^\pi_k(s) \leftarrow \sum_a \pi(a|s) \sum_{s&rsquo;,r} p(s&rsquo;,r|s,a)[r+\gamma V^\pi_{k-1}(s&rsquo;)]$</p>
<p>​		$k=k+1$</p>
<p>​	until $|V^\pi_k(s)-V^\pi_{k-1}(s)| &lt; \epsilon$</p>
<p>Output $V^\pi$</p>
</blockquote>
<p>After getting the state-value of all states we shall optimize the current policy until achieving the optimal policy by choosing the action that can go to the state which has the maximal state-value, which is the <strong>policy improvement</strong> phase:</p>
<blockquote>
<p>Start with a random policy $\pi$</p>
<p>For each $s \in S$</p>
<p>​	$\pi(s) \leftarrow \underset{a}{argmax} \sum_{s&rsquo;,r} p(s&rsquo;,r|s,a)[r+\gamma V^\pi(s&rsquo;)]$</p>
</blockquote>
<p>Therefore, we can get the policy iteration:</p>
<blockquote>
<p>Start with $V_0(s)=0$ for all $s$</p>
<p>Repeat:</p>
<p>​	For each $s \in S$</p>
<p>​		$k=1$</p>
<p>​		Repeat:</p>
<p>​			$V^\pi_k(s) \leftarrow \sum_a \pi(a|s) \sum_{s&rsquo;,r} p(s&rsquo;,r|s,a)[r+\gamma V^\pi_{k-1}(s&rsquo;)]$</p>
<p>​			$k=k+1$</p>
<p>​		until $|V^\pi_k(s)-V^\pi_{k-1}(s)| &lt; \epsilon$</p>
<p>​		$\pi_k(s) \leftarrow \underset{a}{argmax} \sum_{s&rsquo;,r} p(s&rsquo;,r|s,a)[r+\gamma V^\pi(s&rsquo;)]$</p>
<p>until $\pi_k=\pi_{k-1}$</p>
<p>Output $\pi^*$</p>
</blockquote>
<p>Policy iteration requires a lot of time to achieve the optimal policy as it have to calculate state-value every time when updating policies. So we have the value iteration.</p>
<h5 id="22-value-iteration">2.2 Value Iteration</h5>
<p>We can combine the policy evaluation and policy improvement together to get value iteration, by:</p>
<blockquote>
<p>Start with $V_0(s)=0$ for all $s$</p>
<p>$k=1$</p>
<p>Repeat:</p>
<p>​	For each $s \in S$</p>
<p>​		$V^*_k(s) \leftarrow \underset{a}{max} \sum_{s&rsquo;,r} p(s&rsquo;,r|s,a)[r+\gamma V^*_{k-1}(s&rsquo;)]$</p>
<p>​		$\pi^*_k(s) \leftarrow \underset{a}{argmax} \sum_{s&rsquo;,r} p(s&rsquo;,r|s,a)[r+\gamma V^*_{k-1}(s&rsquo;)]$</p>
<p>​		$k=k+1$</p>
<p>until $|V^*_k-V^*_{k-1}| &lt; \epsilon$ and $\pi^*_k=\pi^*_{k-1}$</p>
<p>Output $\pi^*$</p>
</blockquote>
<p>Similarly, we can also get the Q-value iteration:</p>
<blockquote>
<p>Start with $Q_0(s,a)=0$ for all $(s,a)$</p>
<p>$k=1$</p>
<p>Repeat:</p>
<p>​	For each $s \in S$, $a \in A$</p>
<p>​		$Q^*_k(s,a) \leftarrow  \sum_{s&rsquo;,r} p(s&rsquo;,r|s,a)[r+\gamma , \underset{a&rsquo;}{max} , Q^*_{k-1}(s&rsquo;,a&rsquo;)]$</p>
<p>​		$\pi^*_k(s) \leftarrow \underset{a}{argmax} , Q^*_k(s,a)$</p>
<p>​		$k=k+1$</p>
<p>​	until $|Q^*_k-Q^*_{k-1}| &lt; \epsilon$ and $\pi^*_k=\pi^*_{k-1}$</p>
<p>Output $\pi^*$</p>
</blockquote>
<p>At each epoch $t$, in state-value iteration, we have to look for a step forward to compare $v(s_{t+1})$ to make decision, while in action-value iteration, we can make the decision directly by comparing $q(s_t,a_t)$.</p>
<p>Although DP algorithm can find the accurate optimal policy, it have difficulty solving problem with large state space and large action space, not mention the continuous state or action space. And it has to calculate state-value for all states to find the optimal policy which is too time- consuming. Besides, it also requires the transition function of the environment which is very hard in some situations. *</p>
<p>* <strong>Model-free and Model-based</strong>: when applying a specific algorithm to solve MDP, if it doesn&rsquo;t need the transition function of the environment, it is called <strong>model-free</strong> method. On the other hand, if acquiring the transition function is necessary, it is called <strong>model-based</strong> method.</p>
<h3 id="3-monte-carlo-methods-mc">3. Monte Carlo Methods, MC</h3>
<p>DP gets the state-value by solving Bellman equation, which calls for lots of computing power. If we look closer, we would find that to get the state-value $v_\pi(s)=\Bbb{E}_\pi(G(s))$ which is the expectation of $G(s)$, we can use sample to calculate the average value to represent the approximate solution, $\Bbb{E}_\pi(G(s))=\underset{n\rightarrow \infty}{lim}\frac{G_1(s)+G_2(s)+ \cdots +G_n(s)}{n}$, where $G_i(s)$ can be collected by multiple experiments which is **MC control**. The MC model doesn&rsquo;t need the transition function of the environment making it a **model-free** method. However, it needs a  whole episode before updating.</p>
<p>Since we may not be able to access every states at the beginning of a environment, the on-policy first-visit MC control is given as below:  *</p>
<blockquote>
<p>Initialize for all $s \in S$, $a \in A$: $Q(s,a) \leftarrow 0$, $Return(s,a) \leftarrow 0$</p>
<p>Repeat forever:</p>
<p>​	Generate an episode using $\pi$</p>
<p>​	<strong>Update action-value:</strong></p>
<p>​	For each pair (s,a) appearing in the episode:</p>
<p>​		Append the return that follows the first occurrence of (s,a) to $Return(s,a)$</p>
<p>​		$Q(s,a) \leftarrow average(Return(s,a))$</p>
<p>​	<strong>Update policy:</strong></p>
<p>​	$a^* \leftarrow \underset{a}{argmax} , Q(s,a)$</p>
<p>​	$\pi(s) \leftarrow \left{ \begin{aligned} &amp;1-\epsilon+\frac{\epsilon}{|A(s)|} &amp; if ,a=a^* \ &amp;\frac{\epsilon}{|A(s)|} &amp; if , a \neq a^*  \end{aligned} \right.$</p>
</blockquote>
<p>where $\epsilon$ is the probability of applying the random policy, which is  called $\epsilon-greedy$ policy. Since the policy updating phase is being done when the action-value is not exact, the action is not the optimal. So by adding a $\epsilon-soft$ policy, the agent can explore some other action randomly, which will help to get a more accurate action-value.</p>
<p>* <strong>On-policy and Off-policy</strong>: all learning control methods face a dilemma, which is that they seek to learn action-values to find an optimal action but they have to behave non-optimal to find all action-values, in other words, how can they learn optimal action while behaving explorative. <strong>On-policy</strong> methods use the same policy to make decisions and to be evaluated or improved. It made a compromise: it learns a policy near-optimal instead of optimal to maintain some exploration like the $\epsilon - greedy$ policy. <strong>Off-policy</strong> methods use different policies to make decisions and to be evaluated or improved. The one that has been learned is called <strong>target-policy</strong>, and the one that has been used to make decision is called <strong>behavior-policy</strong>. Therefore off-policy methods can learn for the optimal policy while maintaining exploration. This section only introduces the on-policy MC control, off-policy MC control is omitted.</p>
<h3 id="4-temporal-difference-td">4. Temporal Difference, TD</h3>
<p>MC control needs a whole episode which is also time-consuming and in not suitable for infinite MDP. Therefore, in this section, we talk about <strong>TD learning</strong> which takes the advantages of both DP and MC, it is a <strong>model-free</strong> method which utilize experiences like MC and it can learn values from other values like DP making it don&rsquo;t need a whole episode. In other word, the TD learning enables the agent to update its policy without having to acquire the dynamic property of the environment and without having to bootstrap.</p>
<p>This section will introduce the <strong>TD prediction</strong> and two algorithms, namely the on-policy TD control <strong>Sarsa</strong>, and the off-policy TD control <strong>Q-learning</strong>.</p>
<h5 id="41-td-prediction">4.1 TD Prediction</h5>
<p>MC control updates its state-value as:
$$
V(S_t) \leftarrow V(S_t)+\alpha[G_t-V(S_t)]
$$
where $G_t$ is the return of a whole episode meaning it can only update after the whole episode is done.</p>
<p>While TD control update its state-value by:
$$
V(S_t) \leftarrow V(S_t)+\alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_t)]
$$
It updates its state-value $V(S_t)$ in epoch $t+1$ meaning it can update its value after every actions. Define **TD error** as:
$$
\delta_t=R_{t+1}+\gamma V(S_{t+1})-V(S_t)
$$
TD predicts the state-values by TD errors.</p>
<h5 id="42-sarsa">4.2 Sarsa</h5>
<p>Similar to predicting the state-values, action-values can also be predicted by TD error by:
$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t)+\alpha[R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]
$$
This equation means that action-value can be updated at every epoch with (s,a,r,s&rsquo;,a&rsquo;), so the algorithm is called **Sarsa**:</p>
<blockquote>
<p>Initialize for all $s \in S$, $a \in A$: $Q(s,a) \leftarrow 0$</p>
<p>Repeat:</p>
<p>​	Agent makes action $a=\pi(s)$ based on the state $s$ it observe</p>
<p>​	Agent gets reward $r$ and next state $s'$</p>
<p>​	Agent makes action $a'=\pi(s&rsquo;)$ based on the state $s'$ it observe</p>
<p>​	<strong>Update action-value</strong>:</p>
<p>​	$Q(s,a) \leftarrow Q(s,a)+\alpha[r+\gamma Q(s&rsquo;,a&rsquo;)-Q(s,a)]$</p>
<p>​	<strong>Update policy</strong>:</p>
<p>​	$a^* \leftarrow \underset{a}{argmax} , Q(s,a)$</p>
<p>​	update $\pi(s)$ with $\epsilon-greedy$ policy</p>
</blockquote>
<h5 id="43-q-learning">4.3 Q-learning</h5>
<p>Q-learning enables the agent to learn action-value directly to optimal action-value, independent of the policy followed as Sarsa.</p>
<p>The off-policy TD control also called Q-learning is defined as below:
$$
Q(S_t,A_t) \leftarrow Q(S_t,A_t)+\alpha[R_{t+1}+\gamma , \underset{a}{max} , Q(S_{t+1},a)-Q(S_t,A_t)]
$$
Then the algorithm is defined as below:</p>
<blockquote>
<p>Initialize for all $s \in S$, $a \in A$: $Q(s,a) \leftarrow 0$</p>
<p>Repeat:</p>
<p>​	Agent makes action $a=\pi(s)$ based on the state $s$ it observe</p>
<p>​	Agent gets reward $r$ and next state $s'$</p>
<p>​	<strong>Update action-value</strong>:</p>
<p>​	$Q(s,a) \leftarrow Q(s,a)+\alpha[r+\gamma , \underset{a}{max} ,  Q(s&rsquo;,a)-Q(s,a)]$</p>
<p>​	<strong>Update policy</strong>:</p>
<p>​	$a^* \leftarrow \underset{a}{argmax} , Q(s,a)$</p>
<p>​	update $\pi(s)$ with $\epsilon-greedy$ policy</p>
</blockquote>
<p>Q-learning still cannot solve the problem with large state and action spaces.</p>
<p>Q-learning is proposed in 1989, although it has a lot of improvement thereafter, but reinforcement learning algorithms stays relatively still until the breakthrough of DQN introduced in 2015 which will be introduced in next blog. By combining deep learning with reinforcement learning, DRL has greatly influenced the later works.</p>


<footer class="mt-auto d-flex justify-content-center text-muted small secondary_font">
    <span class="text-muted">Copyright (c) 2022, Renhao Zhang,
        <a class="text-muted" href="https://github.com/hadisinaee/avicenna" target="_blank"> created by Avicenna
            (MIT)</a>
    </span>
</footer><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx"
    crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/feather-icons/4.28.0/feather.min.js"></script>
<script>
    feather.replace()
</script></body>

</html>