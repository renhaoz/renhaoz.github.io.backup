<!DOCTYPE html>

<html lang="en-us"><head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha256-4+XzXVhsDmqanXGHaHvgh1gMQKX40OUvDEBTu8JcmNs=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css"
        integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Nanum+Myeongjo&family=Noto+Serif+JP&family=Cormorant+Garamond&family=Libre+Baskerville&family=Source+Serif+Pro&family=Crimson+Text&family=Inter&family=Crimson+Pro&family=Literata&family=Ubuntu+Mono&family=Inter&family=Roboto">
    <link rel="stylesheet" type="text/css" href="/css/style.css">

    
    

    <title>renhaoz | Transformer</title>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA\/G-123123-12', 'auto');
	
	ga('send', 'pageview');
}
</script>


</head><body class="container d-flex flex-column min-vh-100">

<div class="blog_nav_bar secondary_font ">
    
    
    <a class="navbar-brand" href="/">about</a>
    
    
    
    <a class="navbar-brand" href="/blog">« all posts</a>
    
    
</div>



<h3>
    <a class="title" href="/blog/transformer/">Transformer</a>
</h3>

<div class="reading_time secondary_font text-muted ">
    <span>
        Nov 30 2021 · 10 min read
    </span>

</div>




<div class="tags_navigation">
    
    <a class="tag" href="/tags/nlp/">#nlp</a>
    
    <a class="tag" href="/tags/attention/">#attention</a>
    
</div>


<h1 id="transformer">Transformer</h1>
<p><a href="https://arxiv.org/abs/1706.03762">&ldquo;Attention is all you need&rdquo;</a> is a paper published on NIPS2017, conducted by 8 excellent machine learning scientists when they were in Google Brain. It proposed a network architecture, the Transformer, based solely on attention mechanisms, dispensing recurrence and convolutions entirely. It was originally used on translation tasks and achieved SOTA performance. However, it has been later applied in all different tasks in not only NLP but CV as well. It is highly recommended to read the paper, this blog is a simple summary with my own understanding and <a href="https://github.com/harvardnlp/annotated-transformer/blob/master/The%20Annotated%20Transformer.ipynb">Pytorch code</a>.</p>
<p>The official model in Pytorch is:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#  torch.nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation=&lt;function relu&gt;, custom_encoder=None, custom_decoder=None, layer_norm_eps=1e-05, batch_first=False, norm_first=False, device=None, dtype=None)</span>

<span style="color:#75715e">#  Examples:</span>
transformer_model <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Transformer(nhead<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>, num_encoder_layers<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>)
src <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>rand((<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">512</span>))
tgt <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>rand((<span style="color:#ae81ff">20</span>, <span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">512</span>))
out <span style="color:#f92672">=</span> transformer_model(src, tgt)
</code></pre></div><p>To build a Transformer from the beginning, the packages needed are as follow:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> torch
<span style="color:#f92672">import</span> torch.nn <span style="color:#f92672">as</span> nn
<span style="color:#f92672">import</span> torch.nn.functional <span style="color:#f92672">as</span> F
<span style="color:#f92672">import</span> math<span style="color:#f92672">,</span> copy<span style="color:#f92672">,</span> time
<span style="color:#f92672">from</span> torch.autograd <span style="color:#f92672">import</span> Variable
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
</code></pre></div><h3 id="1-model-architecture">1. Model Architecture</h3>
<p><img src="./images/transformer_archi.png" alt=""></p>
<!-- raw HTML omitted -->
<p>At each step, $t$, the model takes both the original sequence $\mathbf{x}=(x_1, x_2, &hellip;x_n)$ and the previously generated symbols $(y_1, y_2, &hellip;y_{t-1})$ as input to generate next symbol $y_t$, until output the final sequence $\mathbf{y}=(y_1, y_2, y_m)$. Therefore, the model is auto-regressive. It is noted that each symbol in sequence $x_t$ is represented by a vector $z_t$ with a dimension of $d_{model}$.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">EncoderDecoder</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        super(EncoderDecoder, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>encoder <span style="color:#f92672">=</span> encoder
        self<span style="color:#f92672">.</span>decoder <span style="color:#f92672">=</span> decoder
        self<span style="color:#f92672">.</span>src_embed <span style="color:#f92672">=</span> src_embed
        self<span style="color:#f92672">.</span>tgt_embed <span style="color:#f92672">=</span> tgt_embed
        self<span style="color:#f92672">.</span>generator <span style="color:#f92672">=</span> generator
    
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">encode</span>(self, src, src_mask):
        <span style="color:#66d9ef">return</span>(self<span style="color:#f92672">.</span>encode(self<span style="color:#f92672">.</span>src_embed(src), src_mask)
               
	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">decode</span>(self, memory, src_mask, tgt, tgt_mask):
		<span style="color:#66d9ef">return</span>(self<span style="color:#f92672">.</span>decoder(self<span style="color:#f92672">.</span>tgt_embed(tgt), mwmory, src_mask, tgt_mask)
        
	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, src, tgt, src_mask, tgt_mask):
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>decode(self<span style="color:#f92672">.</span>encode(src, src_mask), src_mask, tgt, tgt_mask)
               
<span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Generator</span>(nn<span style="color:#f92672">.</span>Module):
	<span style="color:#66d9ef">def</span> __init__(self, d_model, vocab):
		super(Generator, self)<span style="color:#f92672">.</span>__init__()
		self<span style="color:#f92672">.</span>proj <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model, vocab)

	<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
		<span style="color:#66d9ef">return</span> F<span style="color:#f92672">.</span>log_softmax(self<span style="color:#f92672">.</span>proj(x), dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)

</code></pre></div><h4 id="11-encoder-and-decoder">1.1 Encoder and Decoder</h4>
<p>As shown in Fig.1, both the encoder and decoder are composed of $N=6$ identical layers.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">clones</span>(module, N):
    <span style="color:#e6db74">&#34;Produce N identical layers.&#34;</span>
    <span style="color:#66d9ef">return</span> nn<span style="color:#f92672">.</span>ModuleList([copy<span style="color:#f92672">.</span>deepcopy(module) <span style="color:#66d9ef">for</span> _ <span style="color:#f92672">in</span> range(N)])
</code></pre></div><p><strong>Encoder:</strong>   It has two sublayer. The whole process is shown below:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Encoder</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#e6db74">&#34;Core encoder is a stack of N layers&#34;</span>
    <span style="color:#66d9ef">def</span> __init__(self, layer, N):
        super(Encoder, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>layers <span style="color:#f92672">=</span> clones(layer, N)
        self<span style="color:#f92672">.</span>norm <span style="color:#f92672">=</span> LayerNorm(layer<span style="color:#f92672">.</span>size)
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, mask):
        <span style="color:#e6db74">&#34;Pass the input (and mask) through each layer in turn.&#34;</span>
        <span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>layers:
            x <span style="color:#f92672">=</span> layer(x, mask)
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>norm(x)
</code></pre></div><p>The first sublayer is a &lsquo;Multi-Headed Attention&rsquo; followed by a residual layer and a layer normalization (LN).</p>
<p>LN is different from batch normalization (BN), LN can better deal with the problem when samples in a batch have different length.</p>
<p>Let $H$ denote the number of nodes in a hidden layer and $L$ denote the number of the layers, we can calculate the mean $\mu$ and variance $\sigma$.</p>
<p>$$
\mu^l=\frac{1}{H}\sum_{i=1}^{H}x^l_i,, \sigma^l=\sqrt{\frac{1}{H}\sum_{i=1}^{H}(x^l_i-\mu^l)^2}
$$
then, we can get:</p>
<p>$$
\hat{x}^l=\frac{a^l-\mu^l}{(\sigma^l)^2+\sqrt\epsilon}
$$
for each layer, where $\epsilon$ is a little number to prevent denominator to be 0. Besides, there are parameters called gain $g$, and bias $b$ which is like $\gamma$ and $\beta$ in BN. Therefore, we can get output  for $h$ each layers:</p>
<p>$$
h^l=f(g\cdot\frac{a^l-\mu^l}{(\sigma^l)^2+\sqrt\epsilon}+b)
$$
where $f$ is the activation function, the LN process is coded as below:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LayerNorm</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#e6db74">&#34;Construct a layernorm module (See citation for details).&#34;</span>
    <span style="color:#66d9ef">def</span> __init__(self, features, eps<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-6</span>):
        super(LayerNorm, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>a_2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>ones(features))
        self<span style="color:#f92672">.</span>b_2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>zeros(features))
        self<span style="color:#f92672">.</span>eps <span style="color:#f92672">=</span> eps

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        mean <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>mean(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, keepdim<span style="color:#f92672">=</span>True)
        std <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>std(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, keepdim<span style="color:#f92672">=</span>True)
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>a_2 <span style="color:#f92672">*</span> (x <span style="color:#f92672">-</span> mean) <span style="color:#f92672">/</span> (std <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>eps) <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>b_2
</code></pre></div><p>The input of next sublayer is the output of the previous sublayer with residual module, and <code>Dropout</code> is implemented for generalization:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">SublayerConnection</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#e6db74">&#34;&#34;&#34;
</span><span style="color:#e6db74">    A residual connection followed by a layer norm.
</span><span style="color:#e6db74">    Note for code simplicity the norm is first as opposed to last.
</span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
    <span style="color:#66d9ef">def</span> __init__(self, size, dropout):
        super(SublayerConnection, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>norm <span style="color:#f92672">=</span> LayerNorm(size)
        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(dropout)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, sublayer):
        <span style="color:#e6db74">&#34;Apply residual connection to any sublayer with the same size.&#34;</span>
        <span style="color:#66d9ef">return</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>dropout(sublayer(self<span style="color:#f92672">.</span>norm(x)))
</code></pre></div><p>The second sublayer is a FCN followed by a residual layer and a LN. So each sublayer outputs $LN(x+Sublayer(x))$.</p>
<p>Hence, the encoder consisting of &lsquo;Multi-Headed Attention&rsquo; and FCN is modeled as follow:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">EncoderLayer</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#e6db74">&#34;Encoder is made up of self-attn and feed forward (defined below)&#34;</span>
    <span style="color:#66d9ef">def</span> __init__(self, size, self_attn, feed_forward, dropout):
        super(EncoderLayer, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>self_attn <span style="color:#f92672">=</span> self_attn
        self<span style="color:#f92672">.</span>feed_forward <span style="color:#f92672">=</span> feed_forward
        self<span style="color:#f92672">.</span>sublayer <span style="color:#f92672">=</span> clones(SublayerConnection(size, dropout), <span style="color:#ae81ff">2</span>)
        self<span style="color:#f92672">.</span>size <span style="color:#f92672">=</span> size

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, mask):
        <span style="color:#e6db74">&#34;Follow Figure 1 for connections.&#34;</span>
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sublayer[<span style="color:#ae81ff">0</span>](x, <span style="color:#66d9ef">lambda</span> x: self<span style="color:#f92672">.</span>self_attn(x, x, x, mask))
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>sublayer[<span style="color:#ae81ff">1</span>](x, self<span style="color:#f92672">.</span>feed_forward)
</code></pre></div><p><strong>Decoder:</strong>  It has three sublayer. The whole process is shown below:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Decoder</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#e6db74">&#34;Generic N layer decoder with masking.&#34;</span>
    <span style="color:#66d9ef">def</span> __init__(self, layer, N):
        super(Decoder, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>layers <span style="color:#f92672">=</span> clones(layer, N)
        self<span style="color:#f92672">.</span>norm <span style="color:#f92672">=</span> LayerNorm(layer<span style="color:#f92672">.</span>size)
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, memory, src_mask, tgt_mask):
        <span style="color:#66d9ef">for</span> layer <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>layers:
            x <span style="color:#f92672">=</span> layer(x, memory, src_mask, tgt_mask)
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>norm(x)
</code></pre></div><p>The first and second sublayers is similar to the first sublayer in encoder.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DecoderLayer</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#e6db74">&#34;Decoder is made of self-attn, src-attn, and feed forward (defined below)&#34;</span>
    <span style="color:#66d9ef">def</span> __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        super(DecoderLayer, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>size <span style="color:#f92672">=</span> size
        self<span style="color:#f92672">.</span>self_attn <span style="color:#f92672">=</span> self_attn
        self<span style="color:#f92672">.</span>src_attn <span style="color:#f92672">=</span> src_attn
        self<span style="color:#f92672">.</span>feed_forward <span style="color:#f92672">=</span> feed_forward
        self<span style="color:#f92672">.</span>sublayer <span style="color:#f92672">=</span> clones(SublayerConnection(size, dropout), <span style="color:#ae81ff">3</span>)
 
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, memory, src_mask, tgt_mask):
        <span style="color:#e6db74">&#34;Follow Figure 1 for connections.&#34;</span>
        m <span style="color:#f92672">=</span> memory
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sublayer[<span style="color:#ae81ff">0</span>](x, <span style="color:#66d9ef">lambda</span> x: self<span style="color:#f92672">.</span>self_attn(x, x, x, tgt_mask))
        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>sublayer[<span style="color:#ae81ff">1</span>](x, <span style="color:#66d9ef">lambda</span> x: self<span style="color:#f92672">.</span>src_attn(x, m, m, src_mask))
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>sublayer[<span style="color:#ae81ff">2</span>](x, self<span style="color:#f92672">.</span>feed_forward)
</code></pre></div><p>What&rsquo;s different is that the first sublayer&rsquo;s &lsquo;Multi-Headed Attention&rsquo; is masked when training in case of the model seeing the subsequent positions and the second sublayer&rsquo; input consists of the output of the encoder.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">subsequent_mask</span>(size):
    <span style="color:#e6db74">&#34;Mask out subsequent positions.&#34;</span>
    attn_shape <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span>, size, size)
    subsequent_mask <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>triu(np<span style="color:#f92672">.</span>ones(attn_shape), k<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>astype(<span style="color:#e6db74">&#39;uint8&#39;</span>)
    <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>from_numpy(subsequent_mask) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>
</code></pre></div><h4 id="12-attention">1.2 Attention</h4>
<p>The attention mechanism is to map a query and key-value pairs to an output. The output is a weighted <code>value</code> where the weight comes from the compatibility of <code>query</code> and <code>key</code>. A simple form is scaled dot-product attention.</p>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>It is assumed that $Q$ representing <code>query</code> has the same length with $K$ representing <code>key</code>, which is $d_k$. And $V$ representing <code>value</code> has the length of $d_v$. Then, the output is:</p>
<p>$$
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
$$
where the inner product of $Q$ and $K$ represents the compatibility of <code>query</code> and <code>key</code>. The reason of being divided by $\frac{1}{\sqrt{d_k}}$ is that when updating weights, the gradient of $softmax$ can be so small that training can take up a long time, so by being divided by $\frac{1}{\sqrt{d_k}}$ can make the value bigger which will in turn make gradient bigger and training process faster.</p>
<p>*The mask is filled with a very small negative number with will output 0 after $softmax$.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">attention</span>(query, key, value, mask<span style="color:#f92672">=</span>None, dropout<span style="color:#f92672">=</span>None):
    <span style="color:#e6db74">&#34;Compute &#39;Scaled Dot Product Attention&#39;&#34;</span>
    d_k <span style="color:#f92672">=</span> query<span style="color:#f92672">.</span>size(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
    scores <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>matmul(query, key<span style="color:#f92672">.</span>transpose(<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)) \
             <span style="color:#f92672">/</span> math<span style="color:#f92672">.</span>sqrt(d_k)
    <span style="color:#66d9ef">if</span> mask <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        scores <span style="color:#f92672">=</span> scores<span style="color:#f92672">.</span>masked_fill(mask <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1e9</span>)
    p_attn <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(scores, dim <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)
    <span style="color:#66d9ef">if</span> dropout <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        p_attn <span style="color:#f92672">=</span> dropout(p_attn)
    <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>matmul(p_attn, value), p_attn
</code></pre></div><p>Instead of performing a single attention function, the paper found that projecting the $Q$, $ V $, and $K$ to a feature space by a serious of linear function with learned weights is more beneficial.</p>
<p><!-- raw HTML omitted --></p>
<!-- raw HTML omitted -->
<p>Multi-Head mechanism project each vector to $h$ subspaces to focus on different information, which is like different convolution kernel in CNN.</p>
<p>$$
MultiHead(Q,K,V)=Concat(head_1, head_2, &hellip;, head_h)W^o
$$
where</p>
<p>$$
head_i = Attention(QW_i^Q, KW_i^K, VWi^V)
$$
where $W^o, W_i^Q, W_i^K, Wi^V$ are weights of different networks.</p>
<p>$$
W_i^Q\in\Bbb{R}^{d_{model}\times d_k}, W_i^K\in\Bbb{R}^{d_{model}\times d_k}, W_i^V\in\Bbb{R}^{d_{model}\times d_v}, W^o\in\Bbb{R}^{h\cdot d_v\times d_{model}}
$$
in this work, they let $h=8, d_{model}=512, d_k=d_v=8$, it makes the original sequence $1\times d_{model} \rightarrow h\cdot d_v$, where $h\cdot d_v=d_{model}$</p>
<p>It is coded as follow:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MultiHeadedAttention</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, h, d_model, dropout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
        <span style="color:#e6db74">&#34;Take in model size and number of heads.&#34;</span>
        super(MultiHeadedAttention, self)<span style="color:#f92672">.</span>__init__()
        <span style="color:#66d9ef">assert</span> d_model <span style="color:#f92672">%</span> h <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>
        <span style="color:#75715e"># We assume d_v always equals d_k</span>
        self<span style="color:#f92672">.</span>d_k <span style="color:#f92672">=</span> d_model <span style="color:#f92672">//</span> h
        self<span style="color:#f92672">.</span>h <span style="color:#f92672">=</span> h
        self<span style="color:#f92672">.</span>linears <span style="color:#f92672">=</span> clones(nn<span style="color:#f92672">.</span>Linear(d_model, d_model), <span style="color:#ae81ff">4</span>)
        self<span style="color:#f92672">.</span>attn <span style="color:#f92672">=</span> None
        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(p<span style="color:#f92672">=</span>dropout)
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, query, key, value, mask<span style="color:#f92672">=</span>None):
        <span style="color:#e6db74">&#34;Implements Figure 2&#34;</span>
        <span style="color:#66d9ef">if</span> mask <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
            <span style="color:#75715e"># Same mask applied to all h heads.</span>
            mask <span style="color:#f92672">=</span> mask<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)
        nbatches <span style="color:#f92672">=</span> query<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">0</span>)
        
        <span style="color:#75715e"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k </span>
        query, key, value <span style="color:#f92672">=</span> \
            [l(x)<span style="color:#f92672">.</span>view(nbatches, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>h, self<span style="color:#f92672">.</span>d_k)<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)
             <span style="color:#66d9ef">for</span> l, x <span style="color:#f92672">in</span> zip(self<span style="color:#f92672">.</span>linears, (query, key, value))]
        
        <span style="color:#75715e"># 2) Apply attention on all the projected vectors in batch. </span>
        x, self<span style="color:#f92672">.</span>attn <span style="color:#f92672">=</span> attention(query, key, value, mask<span style="color:#f92672">=</span>mask, 
                                 dropout<span style="color:#f92672">=</span>self<span style="color:#f92672">.</span>dropout)
        
        <span style="color:#75715e"># 3) &#34;Concat&#34; using a view and apply a final linear. </span>
        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>transpose(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>contiguous() \
             <span style="color:#f92672">.</span>view(nbatches, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, self<span style="color:#f92672">.</span>h <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>d_k)
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>linears[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>](x)
</code></pre></div><h4 id="13-position-wise-feed-forward-networks">1.3 Position-wise Feed-Forward Networks</h4>
<p>This is just a FCN with two linear transformations and a ReLU activation function.</p>
<p>$$
FFN(x)=max(0,xW_1+b_1)W_2+b_2
$$
The output should be a $1 \times d_{model}$ dimensions  vector representing a word vector.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PositionwiseFeedForward</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#e6db74">&#34;Implements FFN equation.&#34;</span>
    <span style="color:#66d9ef">def</span> __init__(self, d_model, d_ff, dropout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
        super(PositionwiseFeedForward, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>w_1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_model, d_ff)
        self<span style="color:#f92672">.</span>w_2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(d_ff, d_model)
        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(dropout)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>w_2(self<span style="color:#f92672">.</span>dropout(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>w_1(x))))
</code></pre></div><h4 id="14-embeddings-and-softmax">1.4 Embeddings and Softmax</h4>
<p>They use embeddings to covert tokens to vectors.</p>
<p>When embedding, they multiply the weights by $\sqrt{d_{model}}$, because of wanting to make it not differing from the positional information too much which will be introduced in the next chapter.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Embeddings</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self, d_model, vocab):
        super(Embeddings, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>lut <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Embedding(vocab, d_model)
        self<span style="color:#f92672">.</span>d_model <span style="color:#f92672">=</span> d_model

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>lut(x) <span style="color:#f92672">*</span> math<span style="color:#f92672">.</span>sqrt(self<span style="color:#f92672">.</span>d_model)
</code></pre></div><h4 id="15-positional-encoding">1.5 Positional Encoding</h4>
<p>Since the attention mechanism has no recurrence and no convolution, the model cannot use the position information of the sequence. Therefore, before sending the sequence into Transformer, they add a &ldquo;positional encoding&rdquo; on the input.</p>
<p>Assume an input sequence has $n$ tokens and each token are embedded with $d_{model}$ number, so this sequence has the dimension of $n\times d_{model}$, for each position $[pos,2i]$ or$[pos,2i+1]$ , where $pos\in[1,n],i\in[1,d_{model}/2]$, the paper give it a weigh $PE_{(pos,2i)}$ or $PE_{(pos,2i+1)}$ coming out of the positional encoding function.</p>
<p>$$
PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\<br>
PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})
$$
So for each dimension, it corresponds to a unique sinusoid, making the model easily attend the different position of the sequence.</p>
<p>As for why use 10000, there is a guess that is $10000^{1/512}=1.018 \approx 1$. The code is as follow:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PositionalEncoding</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#e6db74">&#34;Implement the PE function.&#34;</span>
    <span style="color:#66d9ef">def</span> __init__(self, d_model, dropout, max_len<span style="color:#f92672">=</span><span style="color:#ae81ff">5000</span>):
        super(PositionalEncoding, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>dropout <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Dropout(p<span style="color:#f92672">=</span>dropout)
        
        <span style="color:#75715e"># Compute the positional encodings once in log space.</span>
        pe <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>zeros(max_len, d_model)
        position <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, max_len)<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">1</span>)
        div_term <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>exp(torch<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, d_model, <span style="color:#ae81ff">2</span>) <span style="color:#f92672">*</span>
                             <span style="color:#f92672">-</span>(math<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">10000.0</span>) <span style="color:#f92672">/</span> d_model))
        pe[:, <span style="color:#ae81ff">0</span>::<span style="color:#ae81ff">2</span>] <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>sin(position <span style="color:#f92672">*</span> div_term)
        pe[:, <span style="color:#ae81ff">1</span>::<span style="color:#ae81ff">2</span>] <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cos(position <span style="color:#f92672">*</span> div_term)
        pe <span style="color:#f92672">=</span> pe<span style="color:#f92672">.</span>unsqueeze(<span style="color:#ae81ff">0</span>)
        self<span style="color:#f92672">.</span>register_buffer(<span style="color:#e6db74">&#39;pe&#39;</span>, pe)
        
    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        x <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> Variable(self<span style="color:#f92672">.</span>pe[:, :x<span style="color:#f92672">.</span>size(<span style="color:#ae81ff">1</span>)], 
                         requires_grad<span style="color:#f92672">=</span>False)
        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>dropout(x)
</code></pre></div><h4 id="16-full-model">1.6 Full Model</h4>
<p>Here, we define a function to  include all the models mentioned above, to make it become a whole Transformer model.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">make_model</span>(src_vocab, tgt_vocab, N<span style="color:#f92672">=</span><span style="color:#ae81ff">6</span>, 
               d_model<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>, d_ff<span style="color:#f92672">=</span><span style="color:#ae81ff">2048</span>, h<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, dropout<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
    <span style="color:#e6db74">&#34;Helper: Construct a model from hyperparameters.&#34;</span>
    c <span style="color:#f92672">=</span> copy<span style="color:#f92672">.</span>deepcopy
    attn <span style="color:#f92672">=</span> MultiHeadedAttention(h, d_model)
    ff <span style="color:#f92672">=</span> PositionwiseFeedForward(d_model, d_ff, dropout)
    position <span style="color:#f92672">=</span> PositionalEncoding(d_model, dropout)
    model <span style="color:#f92672">=</span> EncoderDecoder(
        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
        Decoder(DecoderLayer(d_model, c(attn), c(attn), 
                             c(ff), dropout), N),
        nn<span style="color:#f92672">.</span>Sequential(Embeddings(d_model, src_vocab), c(position)),
        nn<span style="color:#f92672">.</span>Sequential(Embeddings(d_model, tgt_vocab), c(position)),
        Generator(d_model, tgt_vocab))
    
    <span style="color:#75715e"># This was important from their code. </span>
    <span style="color:#75715e"># Initialize parameters with Glorot / fan_avg.</span>
    <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> model<span style="color:#f92672">.</span>parameters():
        <span style="color:#66d9ef">if</span> p<span style="color:#f92672">.</span>dim() <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">1</span>:
            nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>xavier_uniform(p)
    <span style="color:#66d9ef">return</span> model
</code></pre></div><h3 id="2-training">2. Training</h3>
<p>Since I currently don&rsquo;t have 8 GPUs and even with 8 powerful enough GPUs I surely cannot access to them for 3.5 days, here just introduce a few training techniques.</p>
<h4 id="21-optimizer">2.1 Optimizer</h4>
<p>They use the Adam optimizer. And the learning rate is adjusted to:
$$
lr=d_{model}^{-0.5} , min , {step_num^{-0.5}, , step_num*warmup_steps^{-1.5} }
$$
It means that the learning rate will increase at first and decrease after training for a while.</p>
<h4 id="22-regularization">2.2 Regularization</h4>
<ol>
<li>
<p>Residual Dropout</p>
<p>They add <code>Dropout</code> to the end of each sublayer, before it is added to the sub-layer input and normalized.</p>
</li>
<li>
<p>label smoothing</p>
<p>They employed label smoothing to 0.1 which hurts perplexity but improves the accuracy and BLUE scores.</p>
</li>
</ol>
<h3 id="3-other">3. Other</h3>
<p>Attention mechanism has achieved great performance on all different tasks since this paper has published. It had been applied to not only other tasks in NLP, but also in CV. Since it can map the relationship in the whole sequence and not losing the positional information, it can better analyze signals. However, it is just too big making it almost impossible to train by individual and also expensive.</p>


<footer class="mt-auto d-flex justify-content-center text-muted small secondary_font">
    <span class="text-muted">Copyright (c) 2022, Renhao Zhang,
        <a class="text-muted" href="https://github.com/hadisinaee/avicenna" target="_blank"> created by Avicenna
            (MIT)</a>
    </span>
</footer><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx"
    crossorigin="anonymous"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/feather-icons/4.28.0/feather.min.js"></script>
<script>
    feather.replace()
</script></body>

</html>